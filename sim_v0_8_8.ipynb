{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/long2256/PoisonGAN/blob/main/sim_v0_8_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNTidNXQtiy6",
        "outputId": "b01a8933-e3ae-473d-80b2-fee481441cd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flwr_datasets[vision] in /usr/local/lib/python3.10/dist-packages (0.0.2)\n",
            "Requirement already satisfied: datasets<3.0.0,>=2.14.3 in /usr/local/lib/python3.10/dist-packages (from flwr_datasets[vision]) (2.16.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from flwr_datasets[vision]) (1.23.5)\n",
            "Requirement already satisfied: pillow>=6.2.1 in /usr/local/lib/python3.10/dist-packages (from flwr_datasets[vision]) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.14.3->flwr_datasets[vision]) (3.13.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.14.3->flwr_datasets[vision]) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.14.3->flwr_datasets[vision]) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.14.3->flwr_datasets[vision]) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.14.3->flwr_datasets[vision]) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.14.3->flwr_datasets[vision]) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.14.3->flwr_datasets[vision]) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.14.3->flwr_datasets[vision]) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.14.3->flwr_datasets[vision]) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.14.3->flwr_datasets[vision]) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.14.3->flwr_datasets[vision]) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.14.3->flwr_datasets[vision]) (0.20.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.14.3->flwr_datasets[vision]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.14.3->flwr_datasets[vision]) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.14.3->flwr_datasets[vision]) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.14.3->flwr_datasets[vision]) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.14.3->flwr_datasets[vision]) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.14.3->flwr_datasets[vision]) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.14.3->flwr_datasets[vision]) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.14.3->flwr_datasets[vision]) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets<3.0.0,>=2.14.3->flwr_datasets[vision]) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets<3.0.0,>=2.14.3->flwr_datasets[vision]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets<3.0.0,>=2.14.3->flwr_datasets[vision]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets<3.0.0,>=2.14.3->flwr_datasets[vision]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets<3.0.0,>=2.14.3->flwr_datasets[vision]) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets<3.0.0,>=2.14.3->flwr_datasets[vision]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets<3.0.0,>=2.14.3->flwr_datasets[vision]) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets<3.0.0,>=2.14.3->flwr_datasets[vision]) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# depending on your shell, you might need to add `\\` before `[` and `]`.\n",
        "!pip install -q flwr[simulation]\n",
        "!pip install flwr_datasets[vision]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c6e83ac-36fe-4fae-90e5-5e4c0239bdfc",
        "id": "WXOcHlPPtiy8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "e9K5nFNjtiy-",
        "outputId": "bb275efe-0bfa-46e8-ef1b-c82902a6bab1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1429: FutureWarning: The repository for mnist contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mnist\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "from flwr_datasets import FederatedDataset\n",
        "from datasets.utils.logging import disable_progress_bar\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "# Let's set a simulation involving a total of 100 clients\n",
        "NUM_CLIENTS = 33\n",
        "\n",
        "# Download MNIST dataset and partition the \"train\" partition (so one can be assigned to each client)\n",
        "mnist_fds = FederatedDataset(dataset=\"mnist\", partitioners={\"train\": NUM_CLIENTS})\n",
        "# Let's keep the test set as is, and use it to evaluate the global model on the server\n",
        "centralized_testset = mnist_fds.load_full(\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "b_T97AxStiy-"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import ToTensor, Normalize, Compose, Resize\n",
        "\n",
        "\n",
        "def apply_transforms(batch):\n",
        "    \"\"\"Get transformation for MNIST dataset\"\"\"\n",
        "\n",
        "    # transformation to convert images to tensors and apply normalization\n",
        "    transforms = Compose([\n",
        "        ToTensor(),\n",
        "        Normalize((0.1307,), (0.3081,)),\n",
        "        Resize((64, 64), antialias=False)\n",
        "        ])\n",
        "    batch[\"image\"] = [transforms(img) for img in batch[\"image\"]]\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "e7NzdFp6tiy9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, num_classes: int):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=4, stride=2, padding=1)\n",
        "        self.leaky1 = nn.LeakyReLU()\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1)\n",
        "        self.leaky2 = nn.LeakyReLU()\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1)\n",
        "        self.leaky3 = nn.LeakyReLU()\n",
        "\n",
        "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.leaky4 = nn.LeakyReLU()\n",
        "\n",
        "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.leaky5 = nn.LeakyReLU()\n",
        "\n",
        "        self.conv6 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.leaky6 = nn.LeakyReLU()\n",
        "\n",
        "        self.avgpool = nn.AvgPool2d(2, stride=2)\n",
        "\n",
        "        self.fc = nn.Linear(4 * 4 * 128, num_classes)  # 10 classes for MNIST\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.leaky1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.leaky2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.leaky3(x)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        x = self.leaky4(x)\n",
        "\n",
        "        x = self.conv5(x)\n",
        "        x = self.leaky5(x)\n",
        "\n",
        "        x = self.conv6(x)\n",
        "        x = self.leaky6(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output\n",
        "        x = self.fc(x)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=4, stride=2, padding=1)\n",
        "        self.leaky1 = nn.LeakyReLU()\n",
        "        self.dropout = nn.Dropout()\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
        "        self.leaky2 = nn.LeakyReLU()\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
        "        self.leaky3 = nn.LeakyReLU()\n",
        "\n",
        "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.batchnorm3 = nn.BatchNorm2d(128)\n",
        "        self.leaky4 = nn.LeakyReLU()\n",
        "\n",
        "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.batchnorm4 = nn.BatchNorm2d(128)\n",
        "        self.leaky5 = nn.LeakyReLU()\n",
        "\n",
        "        self.conv6 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.leaky6 = nn.LeakyReLU()\n",
        "\n",
        "        self.avgpool = nn.AvgPool2d(2, stride=2)\n",
        "\n",
        "        self.fc = nn.Linear(4 * 4 * 128, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.leaky1(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.leaky2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.leaky3(x)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        x = self.batchnorm3(x)\n",
        "        x = self.leaky4(x)\n",
        "\n",
        "        x = self.conv5(x)\n",
        "        x = self.batchnorm4(x)\n",
        "        x = self.leaky5(x)\n",
        "\n",
        "        x = self.conv6(x)\n",
        "        x = self.leaky6(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output\n",
        "        x = self.fc(x)\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.ConvTranspose2d(100, 256, kernel_size=4, stride=4, padding=0, bias=False)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(256)\n",
        "        self.relu1 = nn.LeakyReLU()\n",
        "\n",
        "        self.conv2 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=4, padding=0, bias=False)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(128)\n",
        "        self.relu2 = nn.LeakyReLU()\n",
        "\n",
        "        self.conv3 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "        self.batchnorm3 = nn.BatchNorm2d(64)\n",
        "        self.relu3 = nn.LeakyReLU()\n",
        "\n",
        "        self.conv4 = nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.batchnorm3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.tanh(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2BkZd5y0tiy-"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "def train(net, trainloader, optim, scheduler, criterion, epochs, device: str):\n",
        "    \"\"\"Train the network on the training set.\"\"\"\n",
        "    net.train()\n",
        "    for _ in range(epochs):\n",
        "        for batch in trainloader:\n",
        "            images, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
        "            optim.zero_grad()\n",
        "            outputs = net(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "        scheduler.step()\n",
        "\n",
        "def test_standard(net, testloader, device: str):\n",
        "    \"\"\"Validate the network on the entire test set.\"\"\"\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    correct, loss = 0, 0.0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[\"image\"].to(device), data[\"label\"].to(device)\n",
        "            outputs = net(images)\n",
        "            loss += criterion(outputs, labels).item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / len(testloader.dataset)\n",
        "    return loss, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EftKsIuMtiy_"
      },
      "outputs": [],
      "source": [
        "import flwr as fl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TkpxfwT9tiy_"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "from typing import Dict, List, Tuple, Union, Optional\n",
        "from flwr.server.client_proxy import ClientProxy\n",
        "from flwr.common import NDArrays, Scalar, Parameters\n",
        "\n",
        "\n",
        "class FlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self, cid, trainloader, valloader, testloader) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.trainloader = trainloader\n",
        "        self.valloader = valloader\n",
        "        self.testloader = testloader\n",
        "        self.cid = cid\n",
        "        self.model = Net(num_classes=10)\n",
        "        self.discriminator = Discriminator()\n",
        "        self.generator = Generator()\n",
        "        # Determine device\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)  # send model to device\n",
        "        self.discriminator.to(self.device)\n",
        "        self.generator.to(self.device)\n",
        "\n",
        "    def set_parameters(self, parameters):\n",
        "        \"\"\"With the model paramters received from the server,\n",
        "        overwrite the uninitialise model in this class with them.\"\"\"\n",
        "\n",
        "        params_dict = zip(self.model.state_dict().keys(), parameters)\n",
        "        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
        "        # now replace the parameters\n",
        "        self.model.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "    def get_parameters(self, config: Dict[str, Scalar]):\n",
        "        \"\"\"Extract all model parameters and conver them to a list of\n",
        "        NumPy arryas. The server doesn't work with PyTorch/TF/etc.\"\"\"\n",
        "        # print(f\"[Client {self.cid}] get_parameters\")\n",
        "        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        \"\"\"This method train the model using the parameters sent by the\n",
        "        server on the dataset of this client. At then end, the parameters\n",
        "        of the locally trained model are communicated back to the server\"\"\"\n",
        "        # print(f\"[Client {self.cid}] fit, config: {config}\")\n",
        "        # copy parameters sent by the server into client's local model\n",
        "        self.set_parameters(parameters)\n",
        "        lr, epochs = config[\"lr\"], config[\"epochs\"]\n",
        "        optim = torch.optim.SGD(self.model.parameters(), lr=lr)\n",
        "        scheduler = lr_scheduler.StepLR(optim, step_size=2, gamma=0.1)\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        train(net=self.model, trainloader=self.trainloader, optim=optim, scheduler=scheduler, criterion=criterion, epochs=epochs, device=self.device)\n",
        "        # return the model parameters to the server as well as extra info (number of training examples in this case)\n",
        "        return self.get_parameters({}), len(self.trainloader), {}\n",
        "\n",
        "    def evaluate(self, parameters: NDArrays, config: Dict[str, Scalar]):\n",
        "        \"\"\"Evaluate the model sent by the server on this client's\n",
        "        local validation set. Then return performance metrics.\"\"\"\n",
        "\n",
        "        self.set_parameters(parameters)\n",
        "        loss, accuracy = test_standard(self.model, self.valloader, device=self.device)\n",
        "\n",
        "        return float(loss), len(self.valloader), {\"accuracy\": accuracy}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "def load_model_state_dict():\n",
        "    net = Net(10)\n",
        "    list_of_files = [fname for fname in glob.glob(\"./model_round_*\")]\n",
        "    latest_round_file = max(list_of_files, key=os.path.getctime)\n",
        "    # latest_round_file = './model_round_df.pth'\n",
        "    print(\"Loading pre-trained model from: \", latest_round_file)\n",
        "    state_dict = torch.load(latest_round_file)\n",
        "    net.load_state_dict(state_dict)\n",
        "    return net"
      ],
      "metadata": {
        "id": "FiR-tVieingX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BPJKeYr6tiy_"
      },
      "outputs": [],
      "source": [
        "def get_evaluate_fn(centralized_testset: Dataset):\n",
        "    \"\"\"This is a function that returns a function. The returned\n",
        "    function (i.e. `evaluate_fn`) will be executed by the strategy\n",
        "    at the end of each round to evaluate the stat of the global\n",
        "    model.\"\"\"\n",
        "\n",
        "    def evaluate_fn(server_round: int, parameters, config):\n",
        "        \"\"\"This function is executed by the strategy it will instantiate\n",
        "        a model and replace its parameters with those from the global model.\n",
        "        The, the model will be evaluate on the test set (recall this is the\n",
        "        whole MNIST test set).\"\"\"\n",
        "\n",
        "        model = Net(num_classes=10)\n",
        "\n",
        "        # Determine device\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model.to(device)  # send model to device\n",
        "\n",
        "        # set parameters to the model\n",
        "        params_dict = zip(model.state_dict().keys(), parameters)\n",
        "        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
        "        model.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "        ###############################################################################\n",
        "        # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        # model = load_model_state_dict()\n",
        "        # model.to(device)\n",
        "        ###############################################################################\n",
        "        # Apply transform to dataset\n",
        "        testset = centralized_testset.with_transform(apply_transforms)\n",
        "\n",
        "        testloader = DataLoader(testset, batch_size=50)\n",
        "        # call test\n",
        "        loss, accuracy = test_standard(model, testloader, device)\n",
        "        print('GLOBAL TEST')\n",
        "        return loss, {\"accuracy\": accuracy}\n",
        "\n",
        "    return evaluate_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uKusgaOAtiy_"
      },
      "outputs": [],
      "source": [
        "from flwr.common import Metrics, FitRes\n",
        "\n",
        "\n",
        "def fit_config(server_round: int) -> Dict[str, Scalar]:\n",
        "    \"\"\"Return a configuration with static batch size and (local) epochs.\"\"\"\n",
        "    config = {\n",
        "        \"epochs\": 10,  # Number of local epochs done by clients\n",
        "        \"lr\": 0.1,  # Learning rate to use by clients during fit()\n",
        "        \"attacker_epochs\": 20,\n",
        "        \"attacker_lr\": 0.05,\n",
        "    }\n",
        "    return config\n",
        "\n",
        "\n",
        "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
        "    \"\"\"Aggregation function for (federated) evaluation metrics, i.e. those returned by\n",
        "    the client's evaluate() method.\"\"\"\n",
        "    # Multiply accuracy of each client by number of examples used\n",
        "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
        "    examples = [num_examples for num_examples, _ in metrics]\n",
        "\n",
        "    # Aggregate and return custom metric (weighted average)\n",
        "    return {\"accuracy\": sum(accuracies) / sum(examples)}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "class SaveModelStrategy(fl.server.strategy.FedAvg):\n",
        "    def aggregate_fit(\n",
        "        self,\n",
        "        server_round: int,\n",
        "        results: List[Tuple[fl.server.client_proxy.ClientProxy, fl.common.FitRes]],\n",
        "        failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
        "    ) -> Tuple[Optional[Parameters], Dict[str, Scalar]]:\n",
        "        \"\"\"Aggregate model weights using weighted average and store checkpoint\"\"\"\n",
        "        model=Net(10)\n",
        "        # Call aggregate_fit from base class (FedAvg) to aggregate parameters and metrics\n",
        "        aggregated_parameters, aggregated_metrics = super().aggregate_fit(server_round, results, failures)\n",
        "\n",
        "        if aggregated_parameters is not None:\n",
        "            print(f\"Saving round {server_round} aggregated_parameters...\")\n",
        "\n",
        "            # Convert `Parameters` to `List[np.ndarray]`\n",
        "            aggregated_ndarrays: List[np.ndarray] = fl.common.parameters_to_ndarrays(aggregated_parameters)\n",
        "\n",
        "            # Convert `List[np.ndarray]` to PyTorch`state_dict`\n",
        "            params_dict = zip(model.state_dict().keys(), aggregated_ndarrays)\n",
        "            state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
        "            model.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "            # Save the model\n",
        "            torch.save(model.state_dict(), f\"model_round_{server_round}.pth\")\n",
        "        return aggregated_parameters, aggregated_metrics"
      ],
      "metadata": {
        "id": "gIoh_OAxATO7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FpB15Xv3tizA"
      },
      "outputs": [],
      "source": [
        "strategy = SaveModelStrategy(\n",
        "    fraction_fit=0.31,  # Sample 10% of available clients for training\n",
        "    fraction_evaluate=0.31,  # Sample 5% of available clients for evaluation\n",
        "    on_fit_config_fn=fit_config,\n",
        "    evaluate_metrics_aggregation_fn=weighted_average,  # aggregates federated metrics\n",
        "    evaluate_fn=get_evaluate_fn(centralized_testset),  # global evaluation function\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "g0HZTdGHtizA"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def get_client_fn(dataset: FederatedDataset):\n",
        "    \"\"\"Return a function to construct a client.\n",
        "\n",
        "    The VirtualClientEngine will execute this function whenever a client is sampled by\n",
        "    the strategy to participate.\n",
        "    \"\"\"\n",
        "\n",
        "    def client_fn(cid: str) -> fl.client.Client:\n",
        "        \"\"\"Construct a FlowerClient with its own dataset partition.\"\"\"\n",
        "\n",
        "        # Let's get the partition corresponding to the i-th client\n",
        "        client_dataset = dataset.load_partition(int(cid), \"train\")\n",
        "\n",
        "        # Now let's split it into train (90%) and validation (10%)\n",
        "        client_dataset_splits = client_dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "        trainset = client_dataset_splits[\"train\"]\n",
        "        valset = client_dataset_splits[\"test\"]\n",
        "\n",
        "        # Now we apply the transform to each batch.\n",
        "        trainloader = DataLoader(\n",
        "            trainset.with_transform(apply_transforms), batch_size=32, shuffle=True\n",
        "        )\n",
        "        valloader = DataLoader(valset.with_transform(apply_transforms), batch_size=32)\n",
        "        testset = centralized_testset.with_transform(apply_transforms)\n",
        "\n",
        "        testloader = DataLoader(testset, batch_size=50)\n",
        "        # Create and return client\n",
        "        return FlowerClient(int(cid), trainloader, valloader, testloader)\n",
        "\n",
        "    return client_fn\n",
        "\n",
        "\n",
        "client_fn_callback = get_client_fn(mnist_fds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRAALrO7tizA"
      },
      "source": [
        "Now we are ready to launch the FL experiment using Flower simulation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WOOq8qkUtizA"
      },
      "outputs": [],
      "source": [
        "# # With a dictionary, you tell Flower's VirtualClientEngine that each\n",
        "# # client needs exclusive access to these many resources in order to run\n",
        "# client_resources = {\"num_cpus\": 0.2, \"num_gpus\": 0.1}\n",
        "\n",
        "# # Let's disable tqdm progress bar in the main thread (used by the server)\n",
        "# disable_progress_bar()\n",
        "\n",
        "# history = fl.simulation.start_simulation(\n",
        "#     client_fn=client_fn_callback,  # a callback to construct a client\n",
        "#     num_clients=NUM_CLIENTS,  # total number of clients in the experiment\n",
        "#     config=fl.server.ServerConfig(num_rounds=70),  # let's run for 10 rounds\n",
        "#     strategy=strategy,  # the strategy that will orchestrate the whole FL pipeline\n",
        "#     client_resources=client_resources,\n",
        "#     actor_kwargs={\n",
        "#         \"on_actor_init_fn\": disable_progress_bar  # disable tqdm on each actor/process spawning virtual clients\n",
        "#     },\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-poMHY3BtizA"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# print(f\"{history.metrics_centralized = }\")\n",
        "\n",
        "# global_accuracy_centralised = history.metrics_centralized[\"accuracy\"]\n",
        "# round = [data[0] for data in global_accuracy_centralised]\n",
        "# acc = [data[1] for data in global_accuracy_centralised]\n",
        "# plt.plot(round, acc)\n",
        "# plt.grid()\n",
        "# plt.ylabel(\"Accuracy (%)\")\n",
        "# plt.xlabel(\"Round\")\n",
        "# plt.title(\"MNIST - IID - 30 clients with 10 clients per round\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dLYQcJ3tizA"
      },
      "source": [
        "Congratulations! With that, you built a Flower client, customized it's instantiation through the `client_fn`, customized the server-side execution through a `FedAvg` strategy configured for this workload, and started a simulation with 100 clients (each holding their own individual partition of the MNIST dataset).\n",
        "\n",
        "Next, you can continue to explore more advanced Flower topics:\n",
        "\n",
        "- Deploy server and clients on different machines using `start_server` and `start_client`\n",
        "- Customize the server-side execution through custom strategies\n",
        "- Customize the client-side execution through `config` dictionaries\n",
        "\n",
        "Get all resources you need!\n",
        "\n",
        "* **[DOCS]** Our complete documenation: https://flower.dev/docs/\n",
        "* **[Examples]** All Flower examples: https://flower.dev/docs/examples/\n",
        "* **[VIDEO]** Our Youtube channel: https://www.youtube.com/@flowerlabs\n",
        "\n",
        "Don't forget to join our Slack channel: https://flower.dev/join-slack/\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_evaluate_fn(centralized_testset: Dataset):\n",
        "    \"\"\"This is a function that returns a function. The returned\n",
        "    function (i.e. `evaluate_fn`) will be executed by the strategy\n",
        "    at the end of each round to evaluate the stat of the global\n",
        "    model.\"\"\"\n",
        "\n",
        "    def evaluate_fn(server_round: int, parameters, config):\n",
        "        \"\"\"This function is executed by the strategy it will instantiate\n",
        "        a model and replace its parameters with those from the global model.\n",
        "        The, the model will be evaluate on the test set (recall this is the\n",
        "        whole MNIST test set).\"\"\"\n",
        "\n",
        "        model = Net(num_classes=10)\n",
        "\n",
        "        # Determine device\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model.to(device)  # send model to device\n",
        "\n",
        "        # set parameters to the model\n",
        "        params_dict = zip(model.state_dict().keys(), parameters)\n",
        "        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
        "        model.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "        ###############################################################################\n",
        "        # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        # model = load_model_state_dict()\n",
        "        # model.to(device)\n",
        "        ###############################################################################\n",
        "        # Apply transform to dataset\n",
        "        testset = centralized_testset.with_transform(apply_transforms)\n",
        "\n",
        "        testloader = DataLoader(testset, batch_size=50)\n",
        "        # call test\n",
        "        loss, accuracy = test(model, testloader, device)\n",
        "        print('GLOBAL TEST')\n",
        "        return loss, {\"accuracy\": accuracy}\n",
        "\n",
        "    return evaluate_fn"
      ],
      "metadata": {
        "id": "dO-w1DtmYldu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "def save_images(images, folder_path, prefix):\n",
        "    # Find the existing files to determine the count\n",
        "    existing_files = glob.glob(os.path.join(folder_path, f\"{prefix}_*.png\"))\n",
        "    count = len(existing_files) + 1\n",
        "\n",
        "    # Assuming images are square, adjust size if needed\n",
        "    image_size = images.shape[1]\n",
        "\n",
        "    # Create a blank canvas for combining images\n",
        "    combined_image = Image.new('L', (4 * image_size, 4 * image_size))\n",
        "\n",
        "    for i, img in enumerate(images):\n",
        "        row = i // 4\n",
        "        col = i % 4\n",
        "        img_pil = Image.fromarray((img * 255).astype('uint8'))  # Convert to PIL image\n",
        "\n",
        "        # Paste the image onto the canvas\n",
        "        combined_image.paste(img_pil, (col * image_size, row * image_size))\n",
        "\n",
        "    # Save the combined image with a dynamic filename\n",
        "    filename = f\"{prefix}_{count}.png\"\n",
        "    combined_image.save(os.path.join(folder_path, filename))\n",
        "\n",
        "def plot_generated(generator, save_folder, num_images=16, device='cuda'):\n",
        "    noise = torch.randn(num_images, 100, 1, 1).to(device)\n",
        "    generated_images = generator(noise)\n",
        "    generated_images = generated_images.squeeze().cpu().detach().numpy()\n",
        "\n",
        "    # Save the combined image\n",
        "    save_images(generated_images, save_folder, \"random_image\")\n"
      ],
      "metadata": {
        "id": "V49PI4f2-7Qm"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_loss_values = []\n",
        "main_acc_values = []\n",
        "standard_loss_values = []\n",
        "standard_acc_values = []"
      ],
      "metadata": {
        "id": "MQyjdNwdAXda"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_other_classes(net, testloader, device: str):\n",
        "    \"\"\"Validate the network on the entire test set excluding class 2.\"\"\"\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    correct_non_poisoned = 0\n",
        "    total_non_poisoned = 0\n",
        "    loss = 0.0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[\"image\"].to(device), data[\"label\"].to(device)\n",
        "\n",
        "            # Exclude class 2\n",
        "            non_poisoned_mask = labels != 2\n",
        "            images_non_poisoned = images[non_poisoned_mask]\n",
        "            labels_non_poisoned = labels[non_poisoned_mask]\n",
        "\n",
        "            output = net(images_non_poisoned)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "\n",
        "            for i in range(len(labels_non_poisoned)):\n",
        "                if pred[i].item() == labels_non_poisoned[i].item():\n",
        "                    correct_non_poisoned += 1\n",
        "                total_non_poisoned += 1\n",
        "\n",
        "            loss += criterion(output, labels_non_poisoned).item()\n",
        "\n",
        "    non_poisoned_accuracy = 100 * correct_non_poisoned / total_non_poisoned if total_non_poisoned != 0 else 0\n",
        "    return loss, non_poisoned_accuracy\n"
      ],
      "metadata": {
        "id": "2LXCVuk-_kpu"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "def test(net, testloader, device: str):\n",
        "    \"\"\"Validate the network on the entire test set.\"\"\"\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    correct_poisoned = 0\n",
        "    total_poisoned = 0\n",
        "    loss = 0.0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[\"image\"].to(device), data[\"label\"].to(device)\n",
        "            output = net(images)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            for i in range(len(labels)):\n",
        "                if labels[i] == 2 and pred[i].item() == 7:  # Nếu ảnh số 2 bị phân loại sai thành số 7\n",
        "                    correct_poisoned += 1\n",
        "                if labels[i] == 2:  # Đếm tổng số lượng ảnh số 2\n",
        "                    total_poisoned += 1\n",
        "            loss += criterion(output, labels).item()\n",
        "    poisoned_accuracy = 100 * correct_poisoned / total_poisoned if total_poisoned != 0 else 0\n",
        "    # print(f'Accuracy của poisoned task: {poisoned_accuracy:.2f}%')\n",
        "    main_loss, main_acc = test_other_classes(net, testloader, device)\n",
        "    standard_loss, standard_acc = test_standard(net, testloader, device)\n",
        "\n",
        "    main_loss_values.append(main_loss)\n",
        "    main_acc_values.append(main_acc)\n",
        "    standard_loss_values.append(standard_loss)\n",
        "    standard_acc_values.append(standard_acc)\n",
        "    return loss, poisoned_accuracy"
      ],
      "metadata": {
        "id": "IDxk25bibt6P"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "def poisontrain(net, generator, discriminator, trainloader,\n",
        "          optimizer_net, optimizer_g, optimizer_d,\n",
        "          scheduler_net, scheduler_g, scheduler_d,\n",
        "          criterion, epochs, device: str):\n",
        "    \"\"\"Train the network on the training set.\"\"\"\n",
        "    net.train()\n",
        "    for _ in range(epochs):\n",
        "        for batch in trainloader:\n",
        "            images, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
        "            batch_size = images.size(0)\n",
        "            # Train discriminator with real images\n",
        "            optimizer_d.zero_grad()\n",
        "            outputs_real = discriminator(images)\n",
        "            real_labels = torch.full((batch_size,), 1, device=device)\n",
        "            loss_real = criterion(outputs_real, real_labels)\n",
        "            loss_real.backward()\n",
        "\n",
        "            # Train discriminator with fake images\n",
        "            noise = torch.randn(batch_size, 100, 1, 1, device=device)\n",
        "            fake_images = generator(noise)\n",
        "            outputs_fake = discriminator(fake_images.detach())\n",
        "            fake_labels = torch.full((batch_size,), 0, device=device)\n",
        "            loss_fake = criterion(outputs_fake, fake_labels)\n",
        "            loss_fake.backward()\n",
        "            d_loss = loss_fake + loss_real\n",
        "            optimizer_d.step()\n",
        "\n",
        "            # Train generator\n",
        "            optimizer_g.zero_grad()\n",
        "            outputs = discriminator(fake_images)\n",
        "            loss_generator = criterion(outputs, real_labels)\n",
        "            loss_generator.backward()\n",
        "            g_loss = loss_generator\n",
        "            optimizer_g.step()\n",
        "\n",
        "            outputs = generator(noise)\n",
        "            predictions = net(outputs)\n",
        "            predicted_labels = torch.max(predictions, dim=1).indices\n",
        "            selected_images = outputs[predicted_labels == 2]\n",
        "            selected_labels = predicted_labels[predicted_labels == 2]\n",
        "            selected_labels[selected_labels == 2] = 7\n",
        "\n",
        "            optimizer_net.zero_grad()\n",
        "            outputs = net(selected_images)\n",
        "            loss = criterion(outputs, selected_labels)\n",
        "            loss.backward()\n",
        "            optimizer_net.step()\n",
        "        scheduler_net.step()\n",
        "        scheduler_g.step()\n",
        "        scheduler_d.step()\n",
        "    save_folder = \"content/images\"\n",
        "\n",
        "    # Tạo thư mục nếu nó chưa tồn tại\n",
        "    os.makedirs(save_folder, exist_ok=True)\n",
        "    plot_generated(generator, save_folder, num_images=16, device=device)"
      ],
      "metadata": {
        "id": "yVvWCzXUgiv-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FlowerClient(FlowerClient):\n",
        "    def fit(self, parameters, config):\n",
        "        \"\"\"This method train the model using the parameters sent by the\n",
        "        server on the dataset of this client. At then end, the parameters\n",
        "        of the locally trained model are communicated back to the server\"\"\"\n",
        "        # print(f\"[Client {self.cid}] fit, config: {config}\")\n",
        "        # copy parameters sent by the server into client's local model\n",
        "        self.set_parameters(parameters)\n",
        "        lr, epochs = config[\"lr\"], config[\"epochs\"]\n",
        "\n",
        "        optimizer_net = torch.optim.SGD(self.model.parameters(), lr=lr)\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        scheduler_net = lr_scheduler.StepLR(optimizer_net, step_size=2, gamma=0.1)\n",
        "        i = 0\n",
        "        if self.cid in [0, 1, 2, 3, 4, 5]:\n",
        "            print('ATTACKER')\n",
        "            attacker_lr, attacker_epochs = config[\"attacker_lr\"], config[\"attacker_epochs\"]\n",
        "            loss, accuracy = test(self.model, self.testloader, device=self.device)\n",
        "\n",
        "            optimizer_g = torch.optim.SGD(self.generator.parameters(), lr=attacker_lr)\n",
        "            scheduler_g = lr_scheduler.StepLR(optimizer_g, step_size=2, gamma=0.1)\n",
        "            optimizer_d = torch.optim.SGD(self.discriminator.parameters(), lr=attacker_lr)\n",
        "            scheduler_d = lr_scheduler.StepLR(optimizer_d, step_size=2, gamma=0.1)\n",
        "\n",
        "            if accuracy > 60:\n",
        "                train(net=self.model, trainloader=self.trainloader, optim=optimizer_net, scheduler=scheduler_net, criterion=criterion, epochs=epochs, device=self.device)\n",
        "                poisontrain(net, self.generator, self.discriminator, self.trainloader,\n",
        "                          optimizer_net, optimizer_g, optimizer_d,\n",
        "                          scheduler_net, scheduler_g, scheduler_d,\n",
        "                          criterion, epochs, self.device)\n",
        "            else:\n",
        "                poisontrain(net, self.generator, self.discriminator, self.trainloader,\n",
        "                          optimizer_net, optimizer_g, optimizer_d,\n",
        "                          scheduler_net, scheduler_g, scheduler_d,\n",
        "                          criterion, epochs, self.device)\n",
        "        else:\n",
        "            train(net=self.model, trainloader=self.trainloader, optim=optimizer_net, scheduler=scheduler_net, criterion=criterion, epochs=epochs, device=self.device)\n",
        "\n",
        "        # return the model parameters to the server as well as extra info (number of training examples in this case)\n",
        "        return self.get_parameters({}), len(self.trainloader), {}\n",
        "\n",
        "    def evaluate(self, parameters: NDArrays, config: Dict[str, Scalar]):\n",
        "        \"\"\"Evaluate the model sent by the server on this client's\n",
        "        local validation set. Then return performance metrics.\"\"\"\n",
        "\n",
        "        self.set_parameters(parameters)\n",
        "        loss, accuracy = test(self.model, self.valloader, device=self.device)\n",
        "\n",
        "        return float(loss), len(self.valloader), {\"accuracy\": accuracy}"
      ],
      "metadata": {
        "id": "UYzulLVecDhh"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "net = Net(10)\n",
        "# list_of_files = [fname for fname in glob.glob(\"./model_round_*\")]\n",
        "# latest_round_file = max(list_of_files, key=os.path.getctime)\n",
        "latest_round_file = './model_round_df.pth'\n",
        "print(\"Loading pre-trained model from: \", latest_round_file)\n",
        "state_dict = torch.load(latest_round_file)\n",
        "net.load_state_dict(state_dict)"
      ],
      "metadata": {
        "id": "nMbxpOIuciXY",
        "outputId": "0310f980-9996-4173-965c-b0f560b2ed47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pre-trained model from:  ./model_round_df.pth\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_parameters():\n",
        "    \"\"\"Extract all model parameters and conver them to a list of\n",
        "    NumPy arryas. The server doesn't work with PyTorch/TF/etc.\"\"\"\n",
        "    return [val.cpu().numpy() for _, val in net.state_dict().items()]"
      ],
      "metadata": {
        "id": "DAGYFxwKnosX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = get_parameters()\n",
        "\n",
        "strategy = SaveModelStrategy(\n",
        "    fraction_fit=0.31,  # Sample 10% of available clients for training\n",
        "    fraction_evaluate=1,  # Sample 5% of available clients for evaluation\n",
        "    on_fit_config_fn=fit_config,\n",
        "    evaluate_metrics_aggregation_fn=weighted_average,  # aggregates federated metrics\n",
        "    evaluate_fn=get_evaluate_fn(centralized_testset),  # global evaluation function\n",
        "    initial_parameters=fl.common.ndarrays_to_parameters(params),\n",
        ")"
      ],
      "metadata": {
        "id": "3Hx55QIkdjce"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# With a dictionary, you tell Flower's VirtualClientEngine that each\n",
        "# client needs exclusive access to these many resources in order to run\n",
        "client_resources = {\"num_cpus\": 2, \"num_gpus\": 1}\n",
        "\n",
        "# Let's disable tqdm progress bar in the main thread (used by the server)\n",
        "disable_progress_bar()\n",
        "history = fl.simulation.start_simulation(\n",
        "    client_fn=client_fn_callback,  # a callback to construct a client\n",
        "    num_clients=NUM_CLIENTS,  # total number of clients in the experiment\n",
        "    config=fl.server.ServerConfig(num_rounds=20),  # let's run for 10 rounds\n",
        "    strategy=strategy,  # the strategy that will orchestrate the whole FL pipeline\n",
        "    client_resources=client_resources,\n",
        "    actor_kwargs={\n",
        "        \"on_actor_init_fn\": disable_progress_bar  # disable tqdm on each actor/process spawning virtual clients\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "kFfMZwVVchgj",
        "outputId": "7c64a957-672e-4938-e541-84e9fddbc461",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO flwr 2024-01-19 05:46:59,307 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=20, round_timeout=None)\n",
            "INFO:flwr:Starting Flower simulation, config: ServerConfig(num_rounds=20, round_timeout=None)\n",
            "2024-01-19 05:47:05,891\tINFO worker.py:1621 -- Started a local Ray instance.\n",
            "INFO flwr 2024-01-19 05:47:08,891 | app.py:213 | Flower VCE: Ray initialized with resources: {'memory': 7664917710.0, 'CPU': 2.0, 'node:172.28.0.12': 1.0, 'GPU': 1.0, 'object_store_memory': 3832458854.0, 'node:__internal_head__': 1.0}\n",
            "INFO:flwr:Flower VCE: Ray initialized with resources: {'memory': 7664917710.0, 'CPU': 2.0, 'node:172.28.0.12': 1.0, 'GPU': 1.0, 'object_store_memory': 3832458854.0, 'node:__internal_head__': 1.0}\n",
            "INFO flwr 2024-01-19 05:47:08,901 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html\n",
            "INFO:flwr:Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html\n",
            "INFO flwr 2024-01-19 05:47:08,908 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 1}\n",
            "INFO:flwr:Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 1}\n",
            "INFO flwr 2024-01-19 05:47:08,988 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 1 actors\n",
            "INFO:flwr:Flower VCE: Creating VirtualClientEngineActorPool with 1 actors\n",
            "INFO flwr 2024-01-19 05:47:08,995 | server.py:89 | Initializing global parameters\n",
            "INFO:flwr:Initializing global parameters\n",
            "INFO flwr 2024-01-19 05:47:09,000 | server.py:272 | Using initial parameters provided by strategy\n",
            "INFO:flwr:Using initial parameters provided by strategy\n",
            "INFO flwr 2024-01-19 05:47:09,006 | server.py:91 | Evaluating initial parameters\n",
            "INFO:flwr:Evaluating initial parameters\n",
            "\u001b[2m\u001b[36m(pid=4837)\u001b[0m 2024-01-19 05:47:21.345145: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[2m\u001b[36m(pid=4837)\u001b[0m 2024-01-19 05:47:21.345211: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[2m\u001b[36m(pid=4837)\u001b[0m 2024-01-19 05:47:21.346816: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[2m\u001b[36m(pid=4837)\u001b[0m 2024-01-19 05:47:23.123984: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO flwr 2024-01-19 05:47:35,202 | server.py:94 | initial parameters (loss, other metrics): 311.81925106048584, {'accuracy': 0.1937984496124031}\n",
            "INFO:flwr:initial parameters (loss, other metrics): 311.81925106048584, {'accuracy': 0.1937984496124031}\n",
            "INFO flwr 2024-01-19 05:47:35,206 | server.py:104 | FL starting\n",
            "INFO:flwr:FL starting\n",
            "DEBUG flwr 2024-01-19 05:47:35,211 | server.py:222 | fit_round 1: strategy sampled 10 clients (out of 33)\n",
            "DEBUG:flwr:fit_round 1: strategy sampled 10 clients (out of 33)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GLOBAL TEST\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG flwr 2024-01-19 05:49:07,364 | server.py:236 | fit_round 1 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 1 received 10 results and 0 failures\n",
            "WARNING flwr 2024-01-19 05:49:07,482 | fedavg.py:242 | No fit_metrics_aggregation_fn provided\n",
            "WARNING:flwr:No fit_metrics_aggregation_fn provided\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving round 1 aggregated_parameters...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO flwr 2024-01-19 05:49:23,284 | server.py:125 | fit progress: (1, 311.8589780330658, {'accuracy': 0.1937984496124031}, 108.0734149650001)\n",
            "INFO:flwr:fit progress: (1, 311.8589780330658, {'accuracy': 0.1937984496124031}, 108.0734149650001)\n",
            "DEBUG flwr 2024-01-19 05:49:23,288 | server.py:173 | evaluate_round 1: strategy sampled 33 clients (out of 33)\n",
            "DEBUG:flwr:evaluate_round 1: strategy sampled 33 clients (out of 33)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GLOBAL TEST\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG flwr 2024-01-19 05:49:36,303 | server.py:187 | evaluate_round 1 received 33 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 1 received 33 results and 0 failures\n",
            "DEBUG flwr 2024-01-19 05:49:36,307 | server.py:222 | fit_round 2: strategy sampled 10 clients (out of 33)\n",
            "DEBUG:flwr:fit_round 2: strategy sampled 10 clients (out of 33)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ATTACKER\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR flwr 2024-01-19 05:50:01,239 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2524, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientException): \u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 191, in fit\n",
            "    return maybe_call_fit(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/client.py\", line 223, in maybe_call_fit\n",
            "    return client.fit(fit_ins)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\n",
            "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
            "  File \"<ipython-input-23-ec0e57cd76ae>\", line 32, in fit\n",
            "  File \"<ipython-input-22-cfa02f428fc2>\", line 18, in poisontrain\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 492, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 251, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 5 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 72, in run\\n    job_results = job_fn(client)\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 191, in fit\\n    return maybe_call_fit(\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/client.py\", line 223, in maybe_call_fit\\n    return client.fit(fit_ins)\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\\n  File \"<ipython-input-23-ec0e57cd76ae>\", line 32, in fit\\n  File \"<ipython-input-22-cfa02f428fc2>\", line 18, in poisontrain\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 492, in backward\\n    torch.autograd.backward(\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 251, in backward\\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2524, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientException): \u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 191, in fit\n",
            "    return maybe_call_fit(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/client.py\", line 223, in maybe_call_fit\n",
            "    return client.fit(fit_ins)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\n",
            "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
            "  File \"<ipython-input-23-ec0e57cd76ae>\", line 32, in fit\n",
            "  File \"<ipython-input-22-cfa02f428fc2>\", line 18, in poisontrain\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 492, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 251, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 5 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 72, in run\\n    job_results = job_fn(client)\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 191, in fit\\n    return maybe_call_fit(\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/client.py\", line 223, in maybe_call_fit\\n    return client.fit(fit_ins)\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\\n  File \"<ipython-input-23-ec0e57cd76ae>\", line 32, in fit\\n  File \"<ipython-input-22-cfa02f428fc2>\", line 18, in poisontrain\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 492, in backward\\n    torch.autograd.backward(\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 251, in backward\\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "\n",
            "ERROR flwr 2024-01-19 05:50:01,248 | ray_client_proxy.py:146 | \u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 191, in fit\n",
            "    return maybe_call_fit(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/client.py\", line 223, in maybe_call_fit\n",
            "    return client.fit(fit_ins)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\n",
            "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
            "  File \"<ipython-input-23-ec0e57cd76ae>\", line 32, in fit\n",
            "  File \"<ipython-input-22-cfa02f428fc2>\", line 18, in poisontrain\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 492, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 251, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 5 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 72, in run\\n    job_results = job_fn(client)\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 191, in fit\\n    return maybe_call_fit(\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/client.py\", line 223, in maybe_call_fit\\n    return client.fit(fit_ins)\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\\n  File \"<ipython-input-23-ec0e57cd76ae>\", line 32, in fit\\n  File \"<ipython-input-22-cfa02f428fc2>\", line 18, in poisontrain\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 492, in backward\\n    torch.autograd.backward(\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 251, in backward\\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "ERROR:flwr:\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 191, in fit\n",
            "    return maybe_call_fit(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/client.py\", line 223, in maybe_call_fit\n",
            "    return client.fit(fit_ins)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\n",
            "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
            "  File \"<ipython-input-23-ec0e57cd76ae>\", line 32, in fit\n",
            "  File \"<ipython-input-22-cfa02f428fc2>\", line 18, in poisontrain\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 492, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 251, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 5 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 72, in run\\n    job_results = job_fn(client)\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 191, in fit\\n    return maybe_call_fit(\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/client.py\", line 223, in maybe_call_fit\\n    return client.fit(fit_ins)\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\\n  File \"<ipython-input-23-ec0e57cd76ae>\", line 32, in fit\\n  File \"<ipython-input-22-cfa02f428fc2>\", line 18, in poisontrain\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 492, in backward\\n    torch.autograd.backward(\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 251, in backward\\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [10,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [15,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [16,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [17,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [18,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [19,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [20,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [21,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [22,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [23,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [24,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [25,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [26,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [27,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [28,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [29,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [30,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "\u001b[2m\u001b[36m(DefaultActor pid=4837)\u001b[0m ../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [31,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
            "ERROR flwr 2024-01-19 05:50:01,367 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2524, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientException): \u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 9 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2524, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientException): \u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 9 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "\n",
            "ERROR flwr 2024-01-19 05:50:01,376 | ray_client_proxy.py:146 | \u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 9 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "ERROR:flwr:\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 9 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "ERROR flwr 2024-01-19 05:50:01,470 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2524, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientException): \u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 17 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2524, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientException): \u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 17 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "\n",
            "ERROR flwr 2024-01-19 05:50:01,475 | ray_client_proxy.py:146 | \u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 17 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "ERROR:flwr:\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 17 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "ERROR flwr 2024-01-19 05:50:01,575 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2524, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientException): \u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 4 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2524, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientException): \u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 4 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "\n",
            "ERROR flwr 2024-01-19 05:50:01,580 | ray_client_proxy.py:146 | \u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 4 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "ERROR:flwr:\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 4 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "ERROR flwr 2024-01-19 05:50:01,681 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2524, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientException): \u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 7 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2524, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientException): \u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 7 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "\n",
            "ERROR flwr 2024-01-19 05:50:01,689 | ray_client_proxy.py:146 | \u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 7 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "ERROR:flwr:\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 7 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "ERROR flwr 2024-01-19 05:50:01,789 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2524, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientException): \u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 10 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2524, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientException): \u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 10 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "\n",
            "ERROR flwr 2024-01-19 05:50:01,803 | ray_client_proxy.py:146 | \u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 10 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "ERROR:flwr:\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 10 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "ERROR flwr 2024-01-19 05:50:01,906 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2524, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientException): \u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 12 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2524, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientException): \u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 12 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "\n",
            "ERROR flwr 2024-01-19 05:50:01,911 | ray_client_proxy.py:146 | \u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 12 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "ERROR:flwr:\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 12 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "ERROR flwr 2024-01-19 05:50:02,018 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2524, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientException): \u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 24 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2524, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientException): \u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 24 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "\n",
            "ERROR flwr 2024-01-19 05:50:02,032 | ray_client_proxy.py:146 | \u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 24 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "ERROR:flwr:\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 24 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "ERROR flwr 2024-01-19 05:50:02,097 | ray_client_proxy.py:145 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2524, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientException): \u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 13 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 138, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 414, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 300, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2524, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientException): \u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 13 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "\n",
            "ERROR flwr 2024-01-19 05:50:02,103 | ray_client_proxy.py:146 | \u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 13 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "ERROR:flwr:\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\n",
            "  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::DefaultActor.run()\u001b[39m (pid=4837, ip=172.28.0.12, actor_id=7b3cac245d2cacfd6a5fca5901000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7898023f5bd0>)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 84, in run\n",
            "    raise ClientException(str(message)) from ex\n",
            "flwr.simulation.ray_transport.ray_actor.ClientException: \n",
            ">>>>>>>A ClientException occurred.('\\n\\tSomething went wrong when running your client workload.\\n\\tClient 13 crashed when the DefaultActor was running its workload.\\n\\tException triggered on the client side: Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 70, in run\\n    client = check_clientfn_returns_client(client_fn(cid))\\n  File \"<ipython-input-14-a3fc0562a22a>\", line 32, in client_fn\\n  File \"<ipython-input-8-76cf27af5d0f>\", line 20, in __init__\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1160, in to\\n    return self._apply(convert)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 810, in _apply\\n    module._apply(fn)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 833, in _apply\\n    param_applied = fn(param)\\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1158, in convert\\n    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\\nRuntimeError: CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\\n',)\n",
            "DEBUG flwr 2024-01-19 05:50:02,113 | server.py:236 | fit_round 2 received 1 results and 9 failures\n",
            "DEBUG:flwr:fit_round 2 received 1 results and 9 failures\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving round 2 aggregated_parameters...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-7326033b98f8>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Let's disable tqdm progress bar in the main thread (used by the server)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdisable_progress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m history = fl.simulation.start_simulation(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mclient_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient_fn_callback\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# a callback to construct a client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mnum_clients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_CLIENTS\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# total number of clients in the experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flwr/simulation/app.py\u001b[0m in \u001b[0;36mstart_simulation\u001b[0;34m(client_fn, num_clients, clients_ids, client_resources, server, config, strategy, client_manager, ray_init_args, keep_initialised, actor_type, actor_kwargs, actor_scheduling)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         hist = run_fl(\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0mserver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitialized_server\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitialized_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flwr/server/app.py\u001b[0m in \u001b[0;36mrun_fl\u001b[0;34m(server, config)\u001b[0m\n\u001b[1;32m    223\u001b[0m ) -> History:\n\u001b[1;32m    224\u001b[0m     \u001b[0;34m\"\"\"Train a model on the given server and return the History object.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m     \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_rounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m     \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"app_fit: losses_distributed %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses_distributed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"app_fit: metrics_distributed_fit %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_distributed_fit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flwr/server/server.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, num_rounds, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# Evaluate model using strategy implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mres_cen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_round\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mres_cen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mloss_cen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics_cen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres_cen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flwr/server/strategy/fedavg.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, server_round, parameters)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mparameters_ndarrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters_to_ndarrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0meval_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserver_round\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters_ndarrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0meval_res\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-4c82cc6226c8>\u001b[0m in \u001b[0;36mevaluate_fn\u001b[0;34m(server_round, parameters, config)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mtestloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# call test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GLOBAL TEST'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-f4494511fdec>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(net, testloader, device)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mpoisoned_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcorrect_poisoned\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_poisoned\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtotal_poisoned\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# print(f'Accuracy của poisoned task: {poisoned_accuracy:.2f}%')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mmain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_other_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mstandard_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstandard_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-bc2c9d3fa055>\u001b[0m in \u001b[0;36mtest_other_classes\u001b[0;34m(net, testloader, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m   2802\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2803\u001b[0m         \u001b[0;34m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2804\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2805\u001b[0m         \u001b[0mn_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2806\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: F811\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m         \u001b[0;34m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2783\u001b[0m         \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2784\u001b[0m         \u001b[0mpa_subtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2785\u001b[0;31m         formatted_output = format_table(\n\u001b[0m\u001b[1;32m   2786\u001b[0m             \u001b[0mpa_subtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2787\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mpython_formatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPythonFormatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat_columns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"column\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mformat_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRowFormat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mformat_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_arrow_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_features_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-69ed425783f6>\u001b[0m in \u001b[0;36mapply_transforms\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         ])\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-69ed425783f6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         ])\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \"\"\"Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\n\u001b[1;32m    127\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mdoes\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msupport\u001b[0m \u001b[0mtorchscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(main_acc_values))# global_accuracy_centralised = history.metrics_centralized[\"accuracy\"]\n",
        "global_accuracy_distributed = history.metrics_distributed[\"accuracy\"]\n",
        "\n",
        "# Rút trích thông tin từ dữ liệu\n",
        "# round_centralised = [data[0] for data in global_accuracy_centralised]\n",
        "round_distributed = [data[0] for data in global_accuracy_distributed]\n",
        "\n",
        "# Vẽ đồ thị\n",
        "# plt.plot(round_centralised, [data[1] for data in global_accuracy_centralised], label=\"Global - Centralised\")\n",
        "plt.plot(range(1, 22), [data[1] for data in global_accuracy_distributed], label=\"Global - Distributed\")\n",
        "plt.plot(range(1, 22), [data[1] for data in  main_acc_values], label=\"Main Task\")\n",
        "plt.plot(range(1, 22), [data[1] for data in standard_acc_values], label=\"Standard Task\")\n",
        "\n",
        "# Thiết lập định dạng của biểu đồ\n",
        "plt.grid()\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.xlabel(\"Round\")\n",
        "plt.title(\"Accuracy Comparison\")\n",
        "plt.legend()\n",
        "xticks_result = plt.xticks(range(1, 21))\n"
      ],
      "metadata": {
        "id": "E33BylpJA_CV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(f\"{history.metrics_centralized = }\")\n",
        "# print(f\"{history.metrics_distributed = }\")\n",
        "\n",
        "\n",
        "global_accuracy_centralised = history.metrics_centralized[\"accuracy\"]\n",
        "# global_accuracy_centralised = history.metrics_distributed[\"accuracy\"]\n",
        "\n",
        "round = [data[0] for data in global_accuracy_centralised]\n",
        "acc = [data[1] for data in global_accuracy_centralised]\n",
        "plt.plot(round, acc)\n",
        "plt.grid()\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.xlabel(\"Round\")\n",
        "plt.title(\"MNIST - IID - 30 clients with 10 clients per round\")\n",
        "xticks_result = plt.xticks(range(1, 21))\n",
        "# plt.yticks(range(0, 100))"
      ],
      "metadata": {
        "id": "NECbOpWgcxdV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}